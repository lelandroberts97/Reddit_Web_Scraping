{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Skiing vs Snowboarding\n",
    "## Problem Statement\n",
    "Mad River Glen is a ski resort in Vermont that has stood their ground on keeping snowboarders off the mountain. Year after year they are pressured by snowboarders to allow them on the mountain. Many proclaim, \"there is no difference between snowboarding and skiing!\" They have asked me, the data scientist on the crew, to prove that there is a difference between skiing and snowboarding using Reddit and natural language processing. \n",
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import praw\n",
    "import time\n",
    "import requests\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a reddit instance\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id = 'Ca5paMO6FqJDUw',\n",
    "#     client_secret = 'JFv1OgKfet5Ud8ePxq0nRz4fnfg',\n",
    "#     username = 'lelandroberts97',\n",
    "#     password = 'Yankees01!',\n",
    "#     user_agent = 'lelandroberts'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our subreddit objects\n",
    "# r_ski = reddit.subreddit('skiing')\n",
    "# r_snowboard = reddit.subreddit('snowboarding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists for the attributes we care about: title, selftext, id, created_utc, author\n",
    "# ski_titles, ski_selftexts, ski_ids, ski_createds, ski_authors = [], [], [], [], []\n",
    "# board_titles, board_selftexts, board_ids, board_createds, board_authors = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRAW: r/skiing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating our sets of documents\n",
    "# for submission in r_ski.hot(limit=1000):\n",
    "#     time.sleep(0.5)\n",
    "#     if submission.is_self == True: # Getting posts with text only\n",
    "#         ski_ids.append(submission.id)\n",
    "#         ski_titles.append(submission.title)\n",
    "#         ski_selftexts.append(submission.selftext)\n",
    "#         ski_createds.append(submission.created)\n",
    "#         ski_authors.append(submission.author)\n",
    "#     if len(ski_ids) >= 500:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a data frame with the ski documents   \n",
    "# ski = pd.DataFrame({\n",
    "#     'id': ski_ids,\n",
    "#     'title': ski_titles,\n",
    "#     'text': ski_selftexts,\n",
    "#     'created': ski_createds,\n",
    "#     'author': ski_authors,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column for class\n",
    "# ski['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ski.head() # Taking a look at the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ski.shape # Checking the size to make sure we have enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRAW: r/snowboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for submission in r_snowboard.hot(limit=1000):\n",
    "#     time.sleep(0.5)\n",
    "#     if submission.is_self == True: # Getting posts with text only\n",
    "#         board_ids.append(submission.id)\n",
    "#         board_titles.append(submission.title)\n",
    "#         board_selftexts.append(submission.selftext)\n",
    "#         board_createds.append(submission.created)\n",
    "#         board_authors.append(submission.author)\n",
    "#     if len(board_ids) >= 500:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with the snowboard documents    \n",
    "# board = pd.DataFrame({\n",
    "#     'id': board_ids,\n",
    "#     'title': board_titles,\n",
    "#     'text': board_selftexts,\n",
    "#     'created': board_createds,\n",
    "#     'author': board_authors,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column for class\n",
    "# board['class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board.head() # Taking a look at the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board.shape # Checking the size to make sure we have enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data frames to csv files\n",
    "# ski.to_csv('../Data/ski.csv', index=False)\n",
    "# board.to_csv('../Data/board.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Scraping with Requests\n",
    "I will use the requests library to see if I can get additional data. Also, as the week progresses, I will be running similar code as above for the 50 most frequent posts and add them to my dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in my original data frame\n",
    "# skiing_0 = pd.read_csv('../Data/ski.csv')\n",
    "# boarding = pd.read_csv('../Data/board.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining urls and user agent\n",
    "# skiing_base_url = 'https://www.reddit.com/r/skiing.json'\n",
    "# boarding_base_url = 'https://www.reddit.com/r/snowboarding.json'\n",
    "\n",
    "# headers={'User-agent':'lrob5178'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests: r/skiing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# Using the requests library to scrape reddit for documents in r/skiing\n",
    "# skiing_post=[]\n",
    "# after=None\n",
    "# for i in range(50):\n",
    "#     if after==None:\n",
    "#         skiing_params={}\n",
    "#     else: skiing_params={'after':after}\n",
    "#     skiing_res=requests.get(skiing_base_url,headers=headers,params=skiing_params)\n",
    "#     if skiing_res.status_code==200:\n",
    "#         skiing_json=skiing_res.json()\n",
    "#         no_blanks=[c['data'] for c in skiing_json['data']['children'] if len(c['data']['selftext'])>10]\n",
    "#         skiing_post.extend(no_blanks)\n",
    "#         after=skiing_json['data']['after']\n",
    "#     else:\n",
    "#         print(skiing_res.status_code)\n",
    "#         break\n",
    "#     time.sleep(1)\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with only the features we are interested in: id, title, author, selftext, and created_utc\n",
    "# skiing = pd.DataFrame(skiing_post).loc[:, ['id', 'title', 'author', 'selftext', 'created_utc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiing['class'] = 1 # Creating a column for the class (our predictor variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 6)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skiing.shape # Checking to see how many documents we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns to match original data frame\n",
    "# col_names = {\n",
    "#     'selftext': 'text',\n",
    "#     'created_utc': 'created'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiing = skiing.rename(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the data scaped using the requests library to the original data frame obtained by using PRAW\n",
    "# skiing_2 = pd.concat([skiing_0, skiing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 6)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skiing_2.shape # Checking the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 6)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skiing_2.drop_duplicates(subset=['text', 'title', 'author']).shape # Checking the size without duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "I did not obtain many additional unique documents from using the requests library. I originally had 220 documents, and, after adding the new documents and dropping the duplicates, I had 238 documents. This makes me think there is a limit to how far you can go back. Instead of just using r/skiing and r/snowboarding, I will try is scraping r/ski and r/snowboard as well, as these are very similar subreddits. For our purposes, there shouldn't be an issue combining them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests: r/ski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# Using the requests library to scrape reddit for documents in r/ski\n",
    "# ski_base_url = 'https://www.reddit.com/r/ski.json'\n",
    "# ski_post=[]\n",
    "# after=None\n",
    "# for i in range(50):\n",
    "#     if after==None:\n",
    "#         ski_params={}\n",
    "#     else: ski_params={'after':after}\n",
    "#     ski_res=requests.get(ski_base_url,headers=headers,params=ski_params)\n",
    "#     if ski_res.status_code==200:\n",
    "#         ski_json=ski_res.json()\n",
    "#         no_blanks=[c['data'] for c in ski_json['data']['children'] if len(c['data']['selftext'])>10]\n",
    "#         ski_post.extend(no_blanks)\n",
    "#         after=ski_json['data']['after']\n",
    "#     else:\n",
    "#         print(ski_res.status_code)\n",
    "#         break\n",
    "#     time.sleep(1)\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with only the features we are interested in: id, title, author, selftext, and created_utc\n",
    "# ski = pd.DataFrame(ski_post).loc[:, ['id', 'title', 'author', 'selftext', 'created_utc']]\n",
    "# board['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590, 5)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ski.shape # Checking the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ski = ski.rename(columns=col_names) # Ranaming columns to match original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ski = ski.drop_duplicates(subset=['text', 'title', 'author']) # Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# ski_2 = pd.concat([skiing_0, ski]) # Combining r/ski and r/skiing data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 6)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ski_2.shape # Checking how many documents we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "675 is a much better number of documents to work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests Library: r/snowboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# Using the requests library to scrape reddit for documents in r/snowboard\n",
    "# board_base_url = 'https://www.reddit.com/r/snowboard.json'\n",
    "# board_post=[]\n",
    "# after=None\n",
    "# for i in range(50):\n",
    "#     if after==None:\n",
    "#         board_params={}\n",
    "#     else: board_params={'after':after}\n",
    "#     board_res=requests.get(board_base_url,headers=headers,params=board_params)\n",
    "#     if board_res.status_code==200:\n",
    "#         board_json=board_res.json()\n",
    "#         no_blanks=[c['data'] for c in board_json['data']['children'] if len(c['data']['selftext'])>10]\n",
    "#         board_post.extend(no_blanks)\n",
    "#         after=board_json['data']['after']\n",
    "#     else:\n",
    "#         print(board_res.status_code)\n",
    "#         break\n",
    "#     time.sleep(1)\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with only the features we are interested in: id, title, author, selftext, and created_utc\n",
    "# board = pd.DataFrame(board_post).loc[:, ['id', 'title', 'author', 'selftext', 'created_utc']]\n",
    "# board['class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 6)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# board.shape # Checking to see how many documents we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board = board.rename(columns=col_names) # Renaming the columns to match the original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board = board.drop_duplicates(subset=['text', 'title', 'author']) # Dropping the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# board_2 = pd.concat([boarding, board]) # Combining the r/snowboard and r/snowboarding data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221, 6)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# board_2.shape # Checking how many documents we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "221 documents is still not enough to work with, especially considering how many we have for the skiing class. I will have to try something else to get more data for the snowboarding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data frames to csv files\n",
    "# ski_2.to_csv('../Data/ski_2.csv', index=False)\n",
    "# board_2.to_csv('../Data/board_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Scraping with Pushshift.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from Raffy\n",
    "def get_date(created):\n",
    "    # get the date of post\n",
    "    return dt.date.fromtimestamp(created)\n",
    "\n",
    "def query_pushshift(subreddit, kind='submission', skip=5, times=50, \n",
    "                    subfield = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'is_self']):\n",
    "    '''\n",
    "    subreddit: The subreddit you want to scrape (required)\n",
    "    kind = 'submission': Scrape the specified post kind\n",
    "    skip = 5: For each iteration of `times`, skip `skip` number of days\n",
    "    times = 50: The number of iterations to make\n",
    "    subfield = long list to type: List of information that is needed\n",
    "    '''\n",
    "    # get the base url that contains information I want to scrape where 'kind' are all submitted posts\n",
    "    # and 'subreddit' is the specified subreddit. Get 500 posts.\n",
    "    stem = f\"https://api.pushshift.io/reddit/search/{kind}/?subreddit={subreddit}&size=500\"\n",
    "    # instantiate list to contain \n",
    "    mylist = []\n",
    "    # scrape posts from the subreddit 'times' times\n",
    "    for x in range(1, times + 1):\n",
    "        # Get posts 'skip' * 'x' days ago\n",
    "        URL = f\"{stem}&after={skip * x}d\"\n",
    "        print(URL)\n",
    "        # Scrape URL\n",
    "        response = requests.get(URL)\n",
    "        # Give me an AssertionError if status code not 200\n",
    "        assert response.status_code == 200\n",
    "        # Of the HTML scraped, take the values of 'data'\n",
    "        the_json=response.json()\n",
    "        no_blanks=[c for c in the_json['data'] if ('selftext' in c.keys()) and len(c['selftext'])>10]\n",
    "        # turn the data into a dataframe\n",
    "        df = pd.DataFrame.from_dict(no_blanks)\n",
    "        # append the dataframe to mylist\n",
    "        mylist.append(df)\n",
    "        # wait to not overrun Reddit's resources\n",
    "        time.sleep(0.5)\n",
    "    # concatenate the dataframes together as one large dataframe, full\n",
    "    full = pd.concat(mylist, sort=False)\n",
    "    if kind == \"submission\":\n",
    "        # take all speficied data\n",
    "        full = full[subfield]\n",
    "        # drop duplicate rows\n",
    "        full = full.drop_duplicates()\n",
    "        full = full.loc[full['is_self'] == True]\n",
    "    # date the the post was... posted\n",
    "    _timestamp = full[\"created_utc\"].apply(get_date)\n",
    "    full['timestamp'] = _timestamp\n",
    "    print(full.shape)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift.io: r/skiing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=5d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=10d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=15d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=20d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=25d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=35d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=40d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=45d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=50d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=55d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=65d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=70d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=75d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=80d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=85d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=95d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=100d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=105d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=110d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=115d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=125d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=130d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=135d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=140d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=145d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=150d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=155d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=160d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=165d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=170d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=175d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=180d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=185d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=190d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=195d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=200d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=205d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=210d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=215d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=220d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=225d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=230d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=235d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=240d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=245d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=skiing&size=500&after=250d\n",
      "(3812, 7)\n"
     ]
    }
   ],
   "source": [
    "# Getting data from r/skiing\n",
    "ski_push = query_pushshift('skiing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski_push['class'] = 1 # Creating a column for our predictor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski_push = ski_push.rename(columns=col_names) # Renaming the columns to match the original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski_push.shape[0] - ski_push.drop_duplicates(subset=['title', 'text', 'author']).shape[0] # Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski_push = ski_push.drop_duplicates(subset=['text', 'title', 'author']) # Dropping the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>author</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First set of skis.</td>\n",
       "      <td>So I am thinking about getting my first skis (...</td>\n",
       "      <td>skiing</td>\n",
       "      <td>1579818807</td>\n",
       "      <td>D3470</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eastern Pennsylvania Skiing</td>\n",
       "      <td>Looking for any advice at all. Or buddies! Goi...</td>\n",
       "      <td>skiing</td>\n",
       "      <td>1579822807</td>\n",
       "      <td>potatoes6</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK Marksman, Bentchetler 100 for a solid daily...</td>\n",
       "      <td>Thanks for the help y’all!</td>\n",
       "      <td>skiing</td>\n",
       "      <td>1579823764</td>\n",
       "      <td>mrrichmahogany</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hank Bilous Massive Transfer &amp;amp; Full Run | ...</td>\n",
       "      <td>[https://www.newschoolers.com/videos/watch/951...</td>\n",
       "      <td>skiing</td>\n",
       "      <td>1579824890</td>\n",
       "      <td>jurassisaurus</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niseko Danger</td>\n",
       "      <td>Just trying to get a general feel for how dang...</td>\n",
       "      <td>skiing</td>\n",
       "      <td>1579827729</td>\n",
       "      <td>IndoorSurvivalist</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                 First set of skis.   \n",
       "1                        Eastern Pennsylvania Skiing   \n",
       "2  OK Marksman, Bentchetler 100 for a solid daily...   \n",
       "3  Hank Bilous Massive Transfer &amp; Full Run | ...   \n",
       "4                                      Niseko Danger   \n",
       "\n",
       "                                                text subreddit     created  \\\n",
       "0  So I am thinking about getting my first skis (...    skiing  1579818807   \n",
       "1  Looking for any advice at all. Or buddies! Goi...    skiing  1579822807   \n",
       "2                         Thanks for the help y’all!    skiing  1579823764   \n",
       "3  [https://www.newschoolers.com/videos/watch/951...    skiing  1579824890   \n",
       "4  Just trying to get a general feel for how dang...    skiing  1579827729   \n",
       "\n",
       "              author  is_self   timestamp  class  \n",
       "0              D3470     True  2020-01-23      1  \n",
       "1          potatoes6     True  2020-01-23      1  \n",
       "2     mrrichmahogany     True  2020-01-23      1  \n",
       "3      jurassisaurus     True  2020-01-23      1  \n",
       "4  IndoorSurvivalist     True  2020-01-23      1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski_push.head() # Viewing the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift.io: r/snowboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=5d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=10d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=15d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=20d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=25d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=35d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=40d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=45d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=50d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=55d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=65d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=70d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=75d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=80d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=85d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=95d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=100d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=105d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=110d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=115d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=125d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=130d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=135d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=140d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=145d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=150d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=155d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=160d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=165d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=170d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=175d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=180d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=185d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=190d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=195d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=200d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=205d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=210d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=215d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=220d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=225d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=230d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=235d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=240d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=245d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=snowboarding&size=500&after=250d\n",
      "(3822, 7)\n"
     ]
    }
   ],
   "source": [
    "# Getting data from r/skiing\n",
    "board_push = query_pushshift('snowboarding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_push['class'] = 0 # Creating a column for our predictor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_push = board_push.rename(columns=col_names) # Renaming the columns to match the original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_push.shape[0] - board_push.drop_duplicates(subset=['title', 'text', 'author']).shape[0] # Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_push = board_push.drop_duplicates(subset=['text', 'title', 'author']) # Dropping the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>author</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buying a board, need opinions on size.</td>\n",
       "      <td>I want to get back into snowboarding, havent d...</td>\n",
       "      <td>snowboarding</td>\n",
       "      <td>1579830441</td>\n",
       "      <td>tfr737</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beginner advice!</td>\n",
       "      <td>So a little context, I used to ski, went ridin...</td>\n",
       "      <td>snowboarding</td>\n",
       "      <td>1579830507</td>\n",
       "      <td>Mcapp-1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Burton snowboarding jacket sizing for a small guy</td>\n",
       "      <td>I am no big guy by American standards: 5'6\", a...</td>\n",
       "      <td>snowboarding</td>\n",
       "      <td>1579835749</td>\n",
       "      <td>bmw-fanboy</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Snowboarder here, what’s the deal with the...</td>\n",
       "      <td>So my wife and I are just getting into snowboa...</td>\n",
       "      <td>snowboarding</td>\n",
       "      <td>1579836431</td>\n",
       "      <td>GoingDigitl</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do FORUM Boards have any value???</td>\n",
       "      <td>like is it worth keeping one?</td>\n",
       "      <td>snowboarding</td>\n",
       "      <td>1579849635</td>\n",
       "      <td>Contentclicker</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0             Buying a board, need opinions on size.   \n",
       "1                                   Beginner advice!   \n",
       "2  Burton snowboarding jacket sizing for a small guy   \n",
       "3  New Snowboarder here, what’s the deal with the...   \n",
       "4                  Do FORUM Boards have any value???   \n",
       "\n",
       "                                                text     subreddit  \\\n",
       "0  I want to get back into snowboarding, havent d...  snowboarding   \n",
       "1  So a little context, I used to ski, went ridin...  snowboarding   \n",
       "2  I am no big guy by American standards: 5'6\", a...  snowboarding   \n",
       "3  So my wife and I are just getting into snowboa...  snowboarding   \n",
       "4                      like is it worth keeping one?  snowboarding   \n",
       "\n",
       "      created          author  is_self   timestamp  class  \n",
       "0  1579830441          tfr737     True  2020-01-23      0  \n",
       "1  1579830507         Mcapp-1     True  2020-01-23      0  \n",
       "2  1579835749      bmw-fanboy     True  2020-01-23      0  \n",
       "3  1579836431     GoingDigitl     True  2020-01-23      0  \n",
       "4  1579849635  Contentclicker     True  2020-01-24      0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_push.head() # Viewing the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "I was able to get nearly 4000 original documents from both r/skiing and r/snowboarding using pushshift.io. I will use this data going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data frames to csv files\n",
    "ski_push.to_csv('../Data/skiing.csv', index=False)\n",
    "board_push.to_csv('../Data/snowboarding.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
